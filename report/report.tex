
\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage[english]{babel}
% Titre complet
\title{Report}

\author{Anna Liednikova}


\begin{document}


\section{Introduction}

 
This project was set within the framework of a collaboration with the ALIAE startup where the aim is to develop a chatbot to collect information from clinical patients. The main idea is to replace strictly defined surveys by a more natural conversation in order to let users express themselves freely so that more information could be collected. First, we should be able to analyse the user utterances (does the user speaks about pain, physical activity etc. ?) and detect at least one most probable intent of user message so that chatbot could choose the right strategy for fulfilling information.


A small manually annotated dataset (roughly 100 sentences for each category) was available. So the purpose of the project is to find word embeddings that will allow to expand it by external data using the existing dataset with labelled intents.

We have tried models for sentence representation by word and the whole context in order to use semi-supervised learning by gradually increasing train data by labelling external sources.

\section{Literature review}

James Ferguson et al faced a similar problem with a small labelled dataset. In their article they describe the approach of automatically expanding dataset and improving the performance of baseline models, that was event trigger identification system in their case.

First, they trained a baseline classifier on available data. Then they identified clusters of additional data to obtain grouped paraphrases inspired by the NewsSpike idea introduced in Zhang et al. (2015). After they labelled clusters with baseline model trained fully-supervised. Combining new labelled data and original one they retrained the event extractor. (Semi-Supervised Event Extraction with Paraphrase Clusters: http://aclweb.org/anthology/N18-2058)



\section{Methodology}

sentence representation, clustering , classification



\subsection{Sentence representation models}

\subsubsection{By Word representation}

The most popular word-basis techniques to represent sentences are LDA (Latent Dirichlet Allocation) and Word2Vector models. We used their implementation from gensim library in order to build models based on the initial labelled dataset.

Preprocessed tokenized sentences were used to create dictionary and were passed to W2V model as it is and in form of Bag-Of-Word to LDA model.

For selecting the best model we changed space dimension (topics for LDA and features for W2W). Preferred number of features for W2V model is 200. It's the start point for our model.

The good point of using this models is that they can be used for both word and sentence representation since one word can be treated as a small sentence.

\subsubsection{By Sentence representation}

In order to use context of words more PositionwiseFeedForward and BiLSTM neural networks were implemenent on Pytorch.



\subsection{Clustering models}

The main idea behind using clustering models for evaluating sentence representations is to detect whatever sentences with the same intent populate the same clusters.

For these purposes following clustering models were used: 
\begin{itemize}
\item K-Means (KM)
\item AgglomerativeClustering(‘ward’) (AG)
\item GaussianMixture (GM)
\end{itemize}

The number of clusters was set to the number of intents (20) that allows using not only purity and Silhouette coefficients to evaluate them but also homogeneity and completeness to get the idea how resulting groups correlate with initial intents. Also, for each model, we plotted a confusion matrix with a background gradient to visualize how well clusters separate different intents.

\subsection{Classification models}

SVC on labelled data, CV

Try RF?


\subsection{Semi-supervised approach}

First, we find the best clusterization for the forum data, later we classify each item and omit ones with low probability of elements at threshold \Theta. In result some clusters will be empty from the beginning and some of them will be ommited due to small population.

Elements of clusters that are left will be labelled by the majority. Usually, they are pure. Later we select N labelled items for each category and extend train data with them to retrain word embedding and classificatin models.

We repeat this cycle until we won't be able to extend train dataset by additional samples or classifier score will stop to grow.


\section{Datasets}

\subsection{Dataset with intents}

The initial dataset is created manually covering 20 main possible users intents. Each sentence represents one intent. For, example,

\begin{lstlisting}
{'text': 'After sleep, for 2-3 hours, 
I am better and then start feeling tired again',
 'intent': 'sleep',
}
\end{lstlisting}

Distribution of labels is show on fig. \ref{figure:name}

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.5]{report1.png}
	\caption{Label distribution in dataset}
 \label{figure:name}
 \end{figure}


{'min': 0, 'max': 12, 'mean': 3.438, 'std': 2.16367}

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.4]{report4.png}
	\caption{Sentence dist}
 \label{words_freq}
 \end{figure}

Total number of sentences is 3305. Found 1882 words after filtering through stopwords list. 

\subsection{Preprocessing routine}

Some proprocessing routine should be covered. This routine includes sentence tokenization, word tokenization, stop words removing and lemmatization. 

Most popular and common tools for nlp are NLTK and SpaCy but they differ in behaviour.


For example, in word tokenization they give different results that can influence not only simple statistics but meaning too. Some example of different tokenization can be seen in table \ref{token_dif}. Though concating words to ones like 'flulike' or '35mg' or 'longterm' sometimes gives more robust and concrete meaning in case of big dataset, in our case it seems better to stay with spacy way of tokenization in order to have smaller and more simple vocabualary.

For stopwords removing there were three options: nltk, spacy and the longest one. The last option was rejected due to containing words like 'want', 'stop', 'successfully' etc. that can be useful for detecting basic intents like positive or negative emotion, social. Finally nltk one was selecting because of containing shorts like 'm' from 'am', 've' from 'have'. Final dictionary contained 1882 words. Also all numbers were changed to num. Chart (\ref{words_freq}) looks fine.

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.4]{report2.png}
	\caption{Words frequency}
 \label{words_freq}
 \end{figure}

Length < 30.


Next problem is empty sentences. But they don't change the dataset much.

intent
fallback           12
disagree           11
hello               4
agree               3
incomprehension     2
positiveEmo         1

array(['what?', 'same that again', 'I am up', 'same', 'can you',
       'do that', 'do this', 'Will do', 'no', 'no i will not',
       "No i don't", 'no', 'no i will not', "No i don't", "What's up?",
       "What's up?", 'he', 'a', 'd', 'i', 'm', 'o', 's', 't', 'y',
       'I will', 'Will do', 'No', 'no', 'No it is not', "No I don't",
       'No', "I'm here"], dtype=object)

\begin{center}
\begin{table}
\begin{tabular}{ |p{7cm}|p{7cm}| }
\hline
NLTK & SpaCy \\ \hline
['i', 'wouldnt', 'go', 'to', 'sleep', 'until', 'like', '5', '6', 'or', '8am'] & 
['i', 'would', 'nt', 'go', 'to', 'sleep', 'until', 'like', '5', '6', 'or', '8', 'am'] \\ \hline
['that', 'is', 'totally', 'wrongheaded'] & ['that', 'is', 'totally', 'wrong', 'headed'] \\ \hline
['i', 'am', 'in', 'the', 'process', 'of', 'tapering', 'from', 'suboxone', 'longterm', 'use'] & 
['i', 'am', 'in', 'the', 'process', 'of', 'tapering', 'from', 'suboxone', 'long', 'term', 'use'] \\ \hline
['i', 'had', 'an', 'onandoff', 'opiateopioid', 'habit', 'from', 'about', '2010'] & 
['i', 'had', 'an', 'on', 'and', 'off', 'opiate', 'opioid', 'habit', 'from', 'about', '2010'] \\ 
\hline
['i', 'have', 'flulike', 'pathologysymptom'] & ['i', 'have', 'flu', 'like', 'pathologysymptom'] \\ \hline
['i', 'have', 'exerciseinduced', 'insomnia'] & ['i', 'have', 'exercise', 'induced', 'insomnia'] \\ \hline
['i', 'm', 'supposed', 'to', 'take', '6', '35mg', 'tablets', 'a', 'day', 'but', 'i', 'have', 'taken', '20', 'today'] & 
['i', 'm', 'supposed', 'to', 'take', '6', '35', 'mg', 'tablets', 'a', 'day', 'but', 'i', 'have', 'taken', '20', 'today'] \\ 
\hline
\end{tabular}	
\caption{\label{token_dif}Tokenization comparision}
\end{table}
\end{center}



\subsection{Forum data}

To improve initial classifier we needed to extend dataset by much more data and more naturally constracted one. No  test  set  is  available was available at the beginning of the project. To  solve this  problem,  we  opted  to  created  our  own  test set following Zhang et al (2015) and Sondhi et al (2010) example.  

Finally, 272552 unique posts from 238 categories. Because our intents are not that precise we should select particular boards with relevant information to be able gradually expand their context. Result is in table \ref{cat_freq}


\begin{center}
\begin{table}
\begin{tabular}{ |p{7cm}|p{7cm}| }
\hline
Category of board |  # of posts \\ \hline
digestive-disorders & 5064 \\ \hline
addiction-recovery & 3644 \\ \hline
sleep-disorders & 1748 \\ \hline
smoking-cessation & 937 \\ \hline
eating-disorder-recovery & 762 \\ \hline
chronic-pain & 735 \\ \hline
chronic-fatigue & 662 \\ \hline
stress & 415 \\ \hline
family-friends-addicts-alcoholics & 312 \\ \hline
pain-management & 25 \\ \hline
\end{tabular}
\caption{\label{cat_freq}Selected categories}

\end{table}
\end{center}


HealthBoards is a medical forum web portal that allows patients to discuss their ailments.
We scraped 272553 unique posts contained in each category. Finally, the corpus consists of N  sentences. Table \ref{visina8} shows the dataset statistics.


{'min': 0, 'max': 2027, 'mean': 10.932588521491454, 'std': 8.964716092053479}
{'min': 3.2580388329566468, 'max': 35.23600634007455, 'mean': 11.224462266261876, 'std': 6.963225985041148}


 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.5]{report3.png}
	\caption{Text stat}\label{visina8}
 \end{figure}

Data should be divided in subsets by increasing sentence length because of the difference in mean values for both datasets.

\subsection{Forum datasets generation}

We should start with the most similar data, so we choose categories of boards that are similar to our intents and after tokenizing  posts into sentences we calculate it's length. For the each iteration we should leave sentences with mean+std words in cleaned text.



\section{Experiment}

\subsection{Clustering}
\subsubsection{Evoluation metrics}

For every model following metrics and their average were calculated: purity score, Silhouette Coefficient, homogeneity and completeness scores. Later for each parameter average among clustering models calculated for each score in order to get both table and plot.

Also for each model confusion matrix created between cluster labels and initial intents.

\subsubsection{W2V model}

Comparision table

\begin{tabular}{ |p{2cm}|p{1cm}|c|c|c|c|p{1cm}| }
\hline
WE & Cluster & purity & Silhouette & homogeneity & complete & Clf CV score \\ \hline
\multirow{3}{*}{Defenders} & KM & & & & &\\
 & AC & & & & &\\
 & GM & & & & &\\ \hline
\multirow{3}{*}{M} & KM & & & & &\\
 & AC & & & & &\\
 & GM & & & & &\\ \hline
Forward & FW & & & & &\\ \hline
\multirow{3}{*}{S} & KM & & & & &\\
 & AC & & & & &\\
 & GM & & & & &\\
\hline
\end{tabular}

\begin{tabular}{ |c|c|c|c|c|c| }
\hline
features num & purity  & silhouette  & homogeneity  & completeness  & AVG\\ \hline 
10  & 0.192032  & 0.0774036  & 0.108761  & 0.127672  & 0.126467\\ \hline 
20  & 0.186283  & 0.061294  & 0.108755  & 0.127291  & 0.120906\\ \hline 
30  & 0.184065  & 0.0630405  & 0.108515  & 0.128856  & 0.121119\\ \hline 
50  & 0.17761  & 0.0662788  & 0.107623  & 0.128358  & 0.119967\\ \hline 
100  & 0.184266  & 0.0653807  & 0.109944  & 0.144074  & 0.125916\\ \hline 
150  & 0.178417  & 0.0566761  & 0.105809  & 0.138289  & 0.119798\\ \hline 
200  & 0.179929  & 0.0496567  & 0.102913  & 0.139742  & 0.11806\\ \hline 
300  & 0.176803  & 0.0505959  & 0.105242  & 0.150409  & 0.120762\\ \hline 
400  & 0.169844  & 0.072453  & 0.0984974  & 0.144132  & 0.121231\\ \hline 
500  & 0.169541  & 0.0478535  & 0.0982757  & 0.15098  & 0.116663\\ \hline 
550  & 0.172264  & 0.0620061  & 0.100714  & 0.153673  & 0.122164\\ \hline 
600  & 0.170045  & 0.0498559  & 0.0988441  & 0.157109  & 0.118964 \\
\hline
\end{tabular}

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.7]{w2v_scores.png}
	\caption{LDA scores}
 \label{words_freq}
 \end{figure}

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.7]{best_w2v_cm.png}
	\caption{LDA confusion matrix}
 \label{words_freq}
 \end{figure}



\subsubsection{LDA model}


\begin{tabular}{ |c|c|c|c|c|c| }
\hline
num & purity  & silhouette  & homogeneity  & completeness  & AVG\\ \hline 
10  & 0.253656  & 0.461589  & 0.157498  & 0.178928  & 0.262918\\ \hline 
20  & 0.259203  & 0.378276  & 0.168191  & 0.182203  & 0.246968\\ \hline 
30  & 0.251538  & 0.331069  & 0.157755  & 0.168448  & 0.227202\\ \hline 
50  & 0.240545  & 0.287032  & 0.147430  & 0.166824  & 0.210458\\ \hline 
100  & 0.266465  & 0.187596  & 0.168647  & 0.189221  & 0.202982\\ \hline 
150  & 0.256077  & 0.167888  & 0.161589  & 0.185054  & 0.192652\\ \hline 
200  & 0.263843  & 0.149212  & 0.175117  & 0.194913  & 0.195771\\ \hline 
300  & 0.250025  & 0.151526  & 0.165607  & 0.184314  & 0.187868\\ \hline 
400  & 0.272718  & 0.192777  & 0.177547  & 0.192270  & 0.208828\\ \hline 
500  & 0.259506  & 0.203725  & 0.174925  & 0.192831  & 0.207747\\ \hline 
550  & 0.229450  & 0.166200  & 0.149270  & 0.158548  & 0.175867\\ \hline 
600  & 0.254261  & 0.191729  & 0.171744  & 0.186001  & 0.200934\\
\hline 
\end{tabular}


 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.7]{lda_scores.png}
	\caption{LDA scores}
 \label{words_freq}
 \end{figure}

 \begin{figure}[h]
 	\centering
 	\includegraphics[scale=0.7]{best_lda_cm.png}
	\caption{LDA confusion matrix}
 \label{words_freq}
 \end{figure}


Best model - 500 topics.


\subsubsection{GloVe pretrained}

\subsubsection{Google News W2V pretrained}

We used word2vec google-news model with 300 dimensional feature space.


CV 0.418 + 0.057%

\subsubsection{CNN by word}

From simple encoder, w2v, lda, w2v + lda

\subsubsection{BiLSTM by word}

From simple encoder, w2v, lda, w2v + lda

\subsubsection{Overall comparision}

\subsection{Classifier}
\subsubsection{Evoluation metrics}


\subsubsection{Results for each model}

\subsection{Semi-supervised learning}

\section{Conclusion}


\section{References}

@InProceedings{N18-2058,
  author = 	"Ferguson, James
		and Lockard, Colin
		and Weld, Daniel
		and Hajishirzi, Hannaneh",
  title = 	"Semi-Supervised Event Extraction with Paraphrase Clusters",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"359--364",
  location = 	"New Orleans, Louisiana",
  url = 	"http://aclweb.org/anthology/N18-2058"
}

@InProceedings{sondhi-EtAl:2010:POSTERS,
  author    = {Sondhi, Parikshit  and  Gupta, Manish  and  Zhai, ChengXiang  and  Hockenmaier, Julia},
  title     = {Shallow Information Extraction from Medical Forum Data},
  booktitle = {Coling 2010: Posters},
  month     = {August},
  year      = {2010},
  address   = {Beijing, China},
  publisher = {Coling 2010 Organizing Committee},
  pages     = {1158--1166},
  url       = {http://www.aclweb.org/anthology/C10-2133}
}


@article{article,
author = {Zhang, Thomas and H D Cho, Jason and Zhai, Chengxiang},
year = {2015},
month = {03},
pages = {},
title = {Understanding User Intents in Online Health Forums},
volume = {19},
journal = {IEEE journal of biomedical and health informatics},
doi = {10.1109/JBHI.2015.2416252}

}

\paragraph{APA}

Zhang, T., Cho, J. H. D., \& Zhai, C. (2015). Understanding User Intents in Online Health Forums. IEEE Journal of Biomedical and Health Informatics, 19(4), 1392-1398. [7066225]. https://doi.org/10.1109/JBHI.2015.2416252

\paragraph{Harvard}

Zhang, T, Cho, JHD \& Zhai, C 2015, 'Understanding User Intents in Online Health Forums' IEEE Journal of Biomedical and Health Informatics, vol. 19, no. 4, 7066225, pp. 1392-1398. https://doi.org/10.1109/JBHI.2015.2416252


\end{document}
